<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning">
  <meta property="og:title" content="Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning"/>
  <meta property="og:description" content="A hybrid quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings"/>
  <meta property="og:url" content="https://sebasmos.github.io/embedding-aware-quantum-svm"/>
  <meta property="og:image" content="static/images/quantum_svm_banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning">
  <meta name="twitter:description" content="A hybrid quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings">
  <meta name="twitter:image" content="static/images/quantum_svm_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="quantum machine learning, quantum support vector machines, vision transformers, quantum kernels, hybrid quantum-classical">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    .quantum-circuit {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      border-radius: 20px;
      padding: 30px;
      margin: 20px 0;
      color: white;
      text-align: center;
    }
    
    .embedding-flow {
      background: #fafbfc;
      border-radius: 20px;
      border: 3px solid #e1e8ed;
      padding: 30px;
      margin: 20px 0;
      position: relative;
    }
    
    .quantum-advantage-box {
      background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
      border-radius: 15px;
      padding: 25px;
      color: white;
      margin: 20px 0;
    }

    h1.title {
      font-family: 'Google Sans', sans-serif;
      font-size: 2.5rem;
      font-weight: 700;
      color: #333;
      text-align: center;
      margin-bottom: 20px;
    }

    h2.title {
      font-family: 'Google Sans', sans-serif;
      font-size: 2rem;
      font-weight: 600;
      color: #444;
      text-align: center;
      margin-bottom: 15px;
      background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo);
      -webkit-background-clip: text;
      background-clip: text;
      -webkit-text-fill-color: transparent;
      line-height: 1.5;
    }

    h3.title {
      font-family: 'Google Sans', sans-serif;
      font-size: 1.5rem;
      font-weight: 500;
      color: #555;
      margin-bottom: 10px;
    }

    p {
      font-family: 'Noto Sans', sans-serif;
      font-size: 1rem;
      line-height: 1.6;
      color: #666;
      margin-bottom: 15px;
      text-align: justify;
    }

    footer.footer {
      background-color: white;
      box-shadow: none;
    }

    .hero .box {
      height: 250px;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      text-align: center;
      word-wrap: break-word;
      padding: 20px;
      box-sizing: border-box;
    }

    .columns {
      display: flex;
      justify-content: space-between;
    }

    table {
      table-layout: fixed;
      width: 100%;
    }

    th, td {
      word-wrap: break-word;
    }

    .performance-table {
      background: white;
      border-radius: 10px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
      padding: 20px;
      margin: 20px 0;
    }

    .best-result {
      background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 50%, #fecfef 100%);
      font-weight: bold;
      border-radius: 5px;
      padding: 2px 4px;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sebasmos777/" target="_blank">Sebasti치n Andr칠s Cajas Ord칩침ez</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="#" target="_blank">Luis Fernando Torres Torres</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="#" target="_blank">Mario Bifulco</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="#" target="_blank">Carlos Andres Duran</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="#" target="_blank">Cristian Bosch</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/ricardo-simon-carbajo-7141464/" target="_blank">Ricardo Simon Carbajo</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>National Irish Centre for AI (CeADAR), University College Dublin (UCD), Dublin, Ireland<br>
              <sup>2</sup>SISTEMIC Research Group, University of Antioquia, Medell칤n, Colombia<br>
              <sup>3</sup>Department of Computer Science, University of Torino, Torino, Italy<br>
              <sup>4</sup>Corporation for Aerospace Initiatives (CASIRI), University of Cauca, Popay치n, Colombia
            </span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2508.00024" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/sebasmos/QuantumVE" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://arxiv.org/abs/2508.00024" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/papers/2508.00024" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 1em; vertical-align: middle;">
                  </span>
                  <span>Hugging Face</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Audio Summary Section -->
<section class="section" style="padding-top: 2rem; padding-bottom: 2rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div style="background: linear-gradient(135deg, #8b5cf6, #a855f7); padding: 2rem; border-radius: 12px; margin-bottom: 1.5rem;">
          <h3 style="color: white; font-size: 1.5rem; font-weight: bold; margin-bottom: 1rem;">
            Main Findings:
          </h3>
          <p style="color: white; font-size: 1.1rem; margin-bottom: 1.5rem;">
            We demonstrate the first systematic evidence that quantum kernel advantage depends critically on embedding choice, revealing fundamental synergy between transformer attention and quantum feature spaces.
          </p>
          <ul style="color: white; font-size: 1rem; margin-left: 1.5rem; margin-bottom: 1.5rem;">
        <li style="margin-bottom: 0.5rem;">ViT embeddings uniquely enable quantum advantage, achieving up to 8.02% accuracy improvements on Fashion-MNIST</li>
            <li style="margin-bottom: 0.5rem;">CNN features show consistent performance degradation in quantum kernels</li>
            <li style="margin-bottom: 0.5rem;">16-qubit tensor network simulation provides scalable quantum machine learning pathway</li>
             </ul>
        
        <audio controls style="width: 100%; max-width: 600px; margin: 0 auto; display: block;">
          <source src="static/audio/summary.mp4" type="audio/mp4">
          Your browser does not support the audio element.
        </audio>
        
        <div class="has-text-centered" style="margin-top: 0.75rem;">
          <p style="color: #adb5bd; font-size: 0.875rem; text-align: center;">
            <i class="fas fa-robot" style="margin-right: 5px;"></i>
            This summary was automatically generated by Google's NotebookLM
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Pipeline Overview Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Hybrid Quantum-Classical Pipeline</h2>
        
        <div class="hero-body">

          <img src="static/images/fig1.png" alt="Pipeline" width="1000" />
          <p><strong> Complete pipeline overview showing the sequential steps from data extraction to QSVM evaluation</strong></p>
          <h4 class="subtitle has-text-centered"><b>Embedding-Aware QSVM Framework.</b></h4>
          The pipeline combines class-balanced k-means distillation with pretrained embeddings, followed by PCA compression and quantum kernel classification using tensor network simulation. This approach reduces complexity from O(70000) to O(1600) kernel evaluations while preserving essential dataset characteristics.
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Quantum Support Vector Machines face scalability challenges due to high-dimensional quantum states and hardware limitations. We propose an <strong>embedding-aware quantum-classical pipeline</strong> combining class-balanced k-means distillation with pretrained Vision Transformer embeddings.
          </p>
          <p>
            Our key finding: <strong>ViT embeddings uniquely enable quantum advantage</strong>, achieving up to 8.02% accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST, while CNN features show performance degradation. Using 16-qubit tensor network simulation via cuTensorNet, we provide the first systematic evidence that quantum kernel advantage depends critically on embedding choice.
          </p>
          <p>
            This reveals fundamental synergy between transformer attention and quantum feature spaces, providing a practical pathway for scalable quantum machine learning that leverages modern neural architectures. The framework demonstrates that achieving quantum advantage requires careful algorithm-representation co-design rather than naive application of quantum methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Key Features -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Key Features</h2>
      <div class="columns">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">丘勇 Quantum-Classical Hybrid</h3>
            <p>Strategic combination of classical preprocessing with quantum kernel methods, enabling scalable quantum machine learning on current hardware.</p>
          </div>
        </div>
        <div class="column">
          <div class="box">
            <h3 class="title is-5">游꿢 Embedding-Aware Design</h3>
            <p>First systematic investigation of how different embedding strategies affect quantum advantage, revealing transformer-quantum synergy.</p>
          </div>
        </div>
        <div class="column">
          <div class="box">
            <h3 class="title is-5">游늳 Proven Quantum Advantage</h3>
            <p>Consistent performance improvements with ViT embeddings: up to 8.02% on Fashion-MNIST and 4.42% on MNIST over classical SVMs.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Quantum Circuit Architecture -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Quantum Circuit Architecture</h2>
        
        <div class="hero-body">
          
          <img src="static/images/fig2.png" alt="Pipeline" width="1000" />
          <p><strong>Quantum circuit diagram showing the 4-qubit parameterized circuit with Hadamard gates, RZ and RY rotations, and CNOT entanglement gates</strong></p>
          <h4 class="subtitle has-text-centered"><b>Data Re-uploading Quantum Feature Map.</b></h4>
          Each qubit is initialized with Hadamard gates, followed by parameterized RZ and RY rotations for data encoding. CNOT gates create entanglement between adjacent qubits, forming an embedding-aware quantum feature map that leverages the exponentially large Hilbert space dimension 2^n.
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Performance Results -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Quantum Advantage with Modern Embeddings</h2>
        
        <div class="performance-table">
          <h4 class="title is-4">Quantum vs Classical SVM Performance Comparison</h4>
          <table class="table is-striped is-hoverable" style="width: 100%; margin: auto;">
            <thead>
              <tr>
                <th>Dataset</th>
                <th>Embedding Type</th>
                <th>Classic SVM Acc</th>
                <th>Quantum SVM Acc</th>
                <th>Quantum Advantage</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="7" style="background-color: #f0f0f0; font-weight: bold;"><strong>MNIST</strong></td>
                <td>Raw Pixels</td>
                <td>0.945</td>
                <td>0.887</td>
                <td>-6.14%</td>
              </tr>
              <tr>
                <td>EffNet-512</td>
                <td>0.969</td>
                <td>0.935</td>
                <td>-3.55%</td>
              </tr>
              <tr>
                <td>EffNet-1536</td>
                <td>0.973</td>
                <td>0.948</td>
                <td>-2.58%</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>ViT-B/32-512</td>
                <td>0.948</td>
                <td><span style="color: blue; font-weight: bold;">0.990</span></td>
                <td><span style="color: green; font-weight: bold;">+4.42%</span></td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>ViT-B/16-512</td>
                <td>0.954</td>
                <td><span style="color: blue; font-weight: bold;">0.995</span></td>
                <td><span style="color: green; font-weight: bold;">+4.25%</span></td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>ViT-L/14</td>
                <td><span style="color: blue; font-weight: bold;">0.983</span></td>
                <td>0.990</td>
                <td><span style="color: green; font-weight: bold;">+0.76%</span></td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>ViT-L/14@336-768</td>
                <td><span style="color: blue; font-weight: bold;">0.984</span></td>
                <td>0.993</td>
                <td><span style="color: green; font-weight: bold;">+0.94%</span></td>
              </tr>
              <tr>
                <td rowspan="7" style="background-color: #f0f0f0; font-weight: bold;"><strong>Fashion-MNIST</strong></td>
                <td>Raw Pixels</td>
                <td>0.783</td>
                <td>0.730</td>
                <td>-6.71%</td>
              </tr>
              <tr>
                <td>EffNet-512</td>
                <td><span style="color: blue; font-weight: bold;">0.917</span></td>
                <td>0.887</td>
                <td>-3.29%</td>
              </tr>
              <tr>
                <td>EffNet-1536</td>
                <td><span style="color: blue; font-weight: bold;">0.916</span></td>
                <td>0.877</td>
                <td>-4.26%</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>ViT-B/32-512</td>
                <td>0.848</td>
                <td><span style="color: blue; font-weight: bold;">0.900</span></td>
                <td><span style="color: green; font-weight: bold;">+6.18%</span></td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>ViT-L/14</td>
                <td>0.871</td>
                <td><span style="color: blue; font-weight: bold;">0.897</span></td>
                <td><span style="color: green; font-weight: bold;">+3.01%</span></td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>ViT-L/14@336-768</td>
                <td>0.865</td>
                <td><span style="color: blue; font-weight: bold;">0.900</span></td>
                <td><span style="color: green; font-weight: bold;">+4.02%</span></td>
              </tr>
            </tbody>
          </table>
        </div>
        
        <h4 class="subtitle has-text-centered">
          <b>Quantum advantage emerges specifically with transformer-based representations, revealing fundamental synergy between quantum kernels and modern neural embeddings.</b>
        </h4>
      </div>
    </div>
  </div>
</section>

<!-- Cross-Validation Analysis -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Comprehensive Cross-Validation Results</h2>
        
        <div class="hero-body">
           <img src="static/images/fig3.png" alt="Pipeline" width="1000" />
          
          <p><strong>Violin plots showing test accuracy distributions for MNIST across K-fold cross-validation</strong></p>
          
           <img src="static/images/fig4.png" alt="Pipeline" width="1000" />
          <p><strong>Violin plots showing test accuracy distributions for Fashion-MNIST across K-fold cross-validation</strong></p>
           <img src="static/images/fig3.png" alt="Pipeline" width="1000" />
          
          <p><strong>Violin plots showing test accuracy distributions for MNIST across K-fold cross-validation</strong></p>
          
           <img src="static/images/fig4.png" alt="Pipeline" width="1000" />
          <p><strong>Violin plots showing test accuracy distributions for Fashion-MNIST across K-fold cross-validation</strong></p>
          <h4 class="subtitle has-text-centered"><b>Stable and Robust Quantum Advantage.</b></h4>
          ViT-based quantum models show consistently higher accuracies and lower variance compared to baselines and EfficientNet-based QSVMs. The narrow, high-accuracy distributions confirm that quantum advantage is stable and reproducible across different data splits.
        </div>

        <div class="performance-table">
          <h4 class="title is-4">Detailed Cross-Validation Performance (Best Models)</h4>
          <table class="table is-striped is-hoverable" style="width: 140%; margin: 0 auto; font-size: 0.8rem;">
            <thead>
              <tr>
                <th>Dataset</th>
                <th>Model</th>
                <th>Test Acc</th>
                <th>Precision</th>
                <th>F1</th>
                <th>AUC</th>
                <th>Time (s)</th>
                <th>Memory (MB)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="8" style="background-color: #f0f0f0; font-weight: bold;">MNIST</td>
                <td>Baseline</td>
                <td>0.882 췀 0.010</td>
                <td>0.887 췀 0.010</td>
                <td>0.882 췀 0.011</td>
                <td>0.990 췀 0.004</td>
                <td>4492.196 췀 39.285</td>
                <td>44116.842 췀 25.978</td>
              </tr>
              <tr>
                <td>Baseline+</td>
                <td>0.884 췀 0.018</td>
                <td>0.888 췀 0.019</td>
                <td>0.884 췀 0.018</td>
                <td>0.991 췀 0.004</td>
                <td>3812.316 췀 42.187</td>
                <td>43537.845 췀 22.515</td>
              </tr>
              <tr>
                <td>QSVM: EffNet-512</td>
                <td>0.889 췀 0.018</td>
                <td>0.893 췀 0.015</td>
                <td>0.889 췀 0.017</td>
                <td>0.992 췀 0.003</td>
                <td>3910.851 췀 25.007</td>
                <td><strong>43506.193 췀 21.365</strong></td>
              </tr>
              <tr>
                <td>QSVM: EffNet-1536</td>
                <td>0.904 췀 0.020</td>
                <td>0.906 췀 0.019</td>
                <td>0.904 췀 0.020</td>
                <td>0.994 췀 0.003</td>
                <td>3819.504 췀 23.488</td>
                <td>43566.972 췀 22.614</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>QSVM: ViT-B/32-512</td>
                <td><span style="color: blue; font-weight: bold;">0.962 췀 0.008</span></td>
                <td>0.963 췀 0.007</td>
                <td>0.962 췀 0.008</td>
                <td>0.999 췀 0.000</td>
                <td>3900.742 췀 24.954</td>
                <td>43510.314 췀 21.536</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>QSVM: ViT-B/16-512</td>
                <td><span style="color: blue; font-weight: bold;">0.973 췀 0.003</span></td>
                <td>0.974 췀 0.003</td>
                <td>0.973 췀 0.003</td>
                <td>0.999 췀 0.000</td>
                <td><strong>3763.170 췀 25.646</strong></td>
                <td>43513.467 췀 20.800</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>QSVM: ViT-L/14</td>
                <td>0.969 췀 0.009</td>
                <td>0.970 췀 0.008</td>
                <td>0.969 췀 0.008</td>
                <td>0.999 췀 0.001</td>
                <td>3816.003 췀 31.957</td>
                <td>43520.979 췀 18.243</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td><strong>QSVM: ViT-L/14@336-768</strong></td>
                <td><span style="color: blue; font-weight: bold;">0.976 췀 0.010</span></td>
                <td><span style="color: blue; font-weight: bold;">0.977 췀 0.010</span></td>
                <td><span style="color: blue; font-weight: bold;">0.975 췀 0.010</span></td>
                <td><span style="color: blue; font-weight: bold;">0.999 췀 0.001</span></td>
                <td>3939.404 췀 24.480</td>
                <td>43520.375 췀 22.726</td>
              </tr>
              <tr>
                <td rowspan="8" style="background-color: #f0f0f0; font-weight: bold;">FashionMNIST</td>
                <td>Baseline</td>
                <td>0.725 췀 0.048</td>
                <td>0.723 췀 0.041</td>
                <td>0.716 췀 0.044</td>
                <td>0.963 췀 0.003</td>
                <td>4456.288 췀 32.991</td>
                <td>44086.054 췀 22.615</td>
              </tr>
              <tr>
                <td>Baseline+</td>
                <td>0.734 췀 0.028</td>
                <td>0.727 췀 0.029</td>
                <td>0.723 췀 0.027</td>
                <td>0.963 췀 0.004</td>
                <td>3803.786 췀 27.142</td>
                <td>43510.356 췀 19.410</td>
              </tr>
              <tr>
                <td>QSVM: EffNet-512</td>
                <td>0.823 췀 0.016</td>
                <td>0.823 췀 0.019</td>
                <td>0.818 췀 0.016</td>
                <td>0.980 췀 0.002</td>
                <td>3797.365 췀 29.575</td>
                <td><strong>43256.111 췀 21.782</strong></td>
              </tr>
              <tr>
                <td>QSVM: EffNet-1536</td>
                <td>0.809 췀 0.022</td>
                <td>0.808 췀 0.020</td>
                <td>0.805 췀 0.020</td>
                <td>0.980 췀 0.004</td>
                <td>3887.396 췀 26.549</td>
                <td>43301.836 췀 17.939</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>QSVM: ViT-B/32-512</td>
                <td>0.818 췀 0.015</td>
                <td>0.821 췀 0.014</td>
                <td>0.816 췀 0.015</td>
                <td>0.981 췀 0.002</td>
                <td>3773.245 췀 25.367</td>
                <td>43250.348 췀 24.488</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>QSVM: ViT-B/16-512</td>
                <td>0.829 췀 0.008</td>
                <td>0.831 췀 0.009</td>
                <td>0.827 췀 0.009</td>
                <td>0.982 췀 0.004</td>
                <td>3853.586 췀 38.180</td>
                <td>43258.243 췀 23.672</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td>QSVM: ViT-L/14</td>
                <td>0.831 췀 0.021</td>
                <td>0.831 췀 0.022</td>
                <td>0.829 췀 0.022</td>
                <td>0.981 췀 0.003</td>
                <td><strong>3766.821 췀 21.742</strong></td>
                <td>43266.337 췀 20.614</td>
              </tr>
              <tr style="background-color: #d4edda;">
                <td><strong>QSVM: ViT-L/14@336-768</strong></td>
                <td><strong>0.841 췀 0.019</strong></td>
                <td><strong>0.841 췀 0.020</strong></td>
                <td><strong>0.840 췀 0.020</strong></td>
                <td><strong>0.983 췀 0.002</strong></td>
                <td>3859.313 췀 20.656</td>
                <td>43265.254 췀 20.394</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Computational Efficiency -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Computational Efficiency Analysis</h2>
        
        <div class="hero-body">
          <!-- Add Figures 5 and 6 here -->

          <img src="static/images/fig5.png" alt="Pipeline" width="1000" />
          
          <p><strong> Runtime vs accuracy comparison for MNIST showing optimal balance between performance and efficiency</strong></p>
          <img src="static/images/fig6.png" alt="Pipeline" width="1000" />
          
          <p><strong> Runtime vs accuracy comparison for Fashion-MNIST showing trade-offs between computational cost and classification performance</strong></p>
          <h4 class="subtitle has-text-centered"><b>Optimal Performance-Efficiency Trade-off.</b></h4>
          ViT-B/16-512 offers the best balance, achieving 97.3% accuracy with fastest runtime (3,763 seconds). Memory usage remains consistent around 43GB across all configurations, demonstrating scalable quantum simulation capabilities.
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Generalization Analysis -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Model Generalization</h2>
        
        <div class="hero-body">
          <!-- Add Figures 7 and 8 here -->

          <img src="static/images/fig7.png" alt="Pipeline" width="1000" />
          <h4 class="subtitle has-text-centered"><b>Strong Cross-Validation to Test Set Alignment.</b></h4>
          Clear diagonal structure with minimal off-diagonal errors confirms robust generalization. The alignment between cross-validation and held-out test results demonstrates that high accuracy reflects true performance across all classes, not overfitting to specific categories.
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Methodology -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Methodology Overview</h2>
      <div class="columns">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">游늵 Data Distillation</h3>
            <p>Class-balanced k-means clustering reduces dataset size from 70,000 to 2,000 samples while preserving representative coverage and eliminating class imbalance effects.</p>
          </div>
        </div>
        <div class="column">
          <div class="box">
            <h3 class="title is-5">游꿢 Embedding Extraction</h3>
            <p>Pretrained EfficientNet-B3 and Vision Transformer variants provide rich semantic features, with PCA compression to match 16-qubit hardware constraints.</p>
          </div>
        </div>
        <div class="column">
          <div class="box">
            <h3 class="title is-5">丘勇 Quantum Kernel</h3>
            <p>Tensor network simulation via cuTensorNet enables efficient quantum kernel computation using data re-uploading techniques, ensuring scalability and accuracy.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


      
    <h2 class="title is-3 has-text-centered">Acknowledgements</h2>
<div class="columns is-centered" style="margin-top: 20px;">
  <div class="column is-narrow" style="margin-right: 20px;">
    <figure class="image is-128x128">
      <img src="static/images/manolo.png" alt="Manolo Logo">
    </figure>
  </div>
  <div class="column is-narrow" style="margin-right: 20px;">
    <figure class="image is-128x128">
      <img src="static/images/ceadar.png" alt="Ceadar Logo">
    </figure>
  </div>
  <div class="column is-narrow">
    <figure class="image is-128x128">
      <img src="static/images/google-cloude.png" alt="Google Cloud Logo">
    </figure>
  </div>
</div>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>. 
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>